{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b631c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Set the directory\n",
    "directory = \"2017_j\"\n",
    "target_subreddits = [\"wallstreetbets\"]\n",
    "\n",
    "# Create an empty dataframe\n",
    "df = pd.DataFrame(columns=[\"subreddit\", \"author\", \"created_utc\", \"parent_id\", \"id\", \"body\"])\n",
    "\n",
    "\n",
    "# Iterate over the files in ascending order by filename, set number of files to process\n",
    "files = sorted([f for f in os.listdir(directory)])\n",
    "files = files[:25]\n",
    "print(files)\n",
    "\n",
    "for file in tqdm(files):\n",
    "    with open(f\"{directory}/{file}\", \"r\") as f:\n",
    "        tmpdf = pd.DataFrame([json.loads(line) for line in f])\n",
    "        # filter out the rows that do not have the target subreddit\n",
    "        tmpdf = tmpdf[tmpdf[\"subreddit\"].isin(target_subreddits)]\n",
    "        # print(tmpdf.head())\n",
    "        df = pd.concat([df, tmpdf[[\"subreddit\", \"author\", \"created_utc\", \"parent_id\", \"id\", \"body\"]]])\n",
    "\n",
    "        print(df.head())\n",
    "        print('length:', len(df))\n",
    "        df = df.sort_values(by=\"created_utc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id']\n",
    "\n",
    "for id in df['id']:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    latest_body = row[\"body\"]\n",
    "    latest_author = row[\"author\"]\n",
    "    latest_id = row[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/peft/quicktour \n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"Sentdex/Walls1337bot-Llama2-7B-003.005.5000\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c82f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorama\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(50*\"=\")\n",
    "    formatted_prompt = f\"### BEGIN CONVERSATION ###\\n\\n## Speaker_0: ##\\n{prompt}\\n\\n## Walls1337bot: ##\\n\"\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=128)\n",
    "    output_text = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
    "    just_response = output_text.split(formatted_prompt)[1].split(\"\\n\\n##\")[0]\n",
    "    \n",
    "    #print(formatted_prompt+just_response)\n",
    "    print(f\"{colorama.Fore.CYAN}{formatted_prompt}{colorama.Style.RESET_ALL}{colorama.Fore.YELLOW}{just_response}{colorama.Style.RESET_ALL}\")\n",
    "    print(50*\"=\")\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
